{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d300794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937032fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/dataset_penyisihan_bdc_2024.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334fb2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93210b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c7ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['label'].unique())\n",
    "print(data.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f16a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "# Fungsi membersihkan teks tanpa stemming/lemmatisasi\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "\n",
    "    # 1. Hapus RT, via, cc di awal\n",
    "    text = re.sub(r'^(RT|rt|via|cc)\\b', '', text).strip()\n",
    "\n",
    "    # 2. Hapus mention @username\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # 3. Hapus URL\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # 4. Hapus hashtag\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "\n",
    "    # 5. Hapus bracket [RE ...] atau yang sejenis\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "\n",
    "    # 6. Hapus encoding random (+ECNv...= dsb)\n",
    "    text = re.sub(r'\\S*=\\S*', '', text)\n",
    "\n",
    "    # 7. Hapus karakter non-ASCII dan simbol aneh\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "    # 8. Normalisasi unicode (hilangkan diakritik tak perlu)\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "    return text\n",
    "\n",
    "# Terapkan ke kolom data\n",
    "data['clean_text'] = data['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ef064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hapus baris jika kolom 'clean_text' kosong atau hanya berisi spasi\n",
    "data = data[data['clean_text'].astype(str).str.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17efc097",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29b4bf0",
   "metadata": {},
   "source": [
    "# AUGMENTASI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed0c583",
   "metadata": {},
   "source": [
    "## Sosial Budaya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5960a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "label_target = \"Sosial Budaya\"\n",
    "output_file = f\"prepData_{label_target.replace(' ', '_')}.csv\"\n",
    "\n",
    "majority_class = data['label'].value_counts().idxmax()\n",
    "target_count = data['label'].value_counts()[majority_class]\n",
    "\n",
    "subset = data[data['label'] == label_target]\n",
    "count = len(subset)\n",
    "needed = target_count - count\n",
    "print(f\"Augmentasi kelas {label_target} -> {needed} data\")\n",
    "\n",
    "if needed <= 0:\n",
    "    raise SystemExit(\"Tidak perlu augmentasi.\")\n",
    "\n",
    "# =========================\n",
    "# Device Setting (GPU Kaggle)\n",
    "# =========================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ“Œ Menggunakan device: {device}\")\n",
    "\n",
    "# =========================\n",
    "# Load Models\n",
    "# =========================\n",
    "model_path = \"data/indoT5-paraphrase\"\n",
    "tokenizer_para = AutoTokenizer.from_pretrained(model_path)\n",
    "model_para = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "sim_model = SentenceTransformer('data/distiluse', device=device)\n",
    "\n",
    "# =========================\n",
    "# Fungsi Paraphrase IndoT5\n",
    "# =========================\n",
    "def paraphrase_text(text):\n",
    "    inputs = tokenizer_para(\n",
    "        f\"parafrase: {text}\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model_para.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=256,\n",
    "        num_return_sequences=3,\n",
    "        do_sample=True,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "    candidates = tokenizer_para.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    embeddings = sim_model.encode([text] + candidates, convert_to_tensor=True, device=device)\n",
    "    similarities = util.cos_sim(embeddings[0], embeddings[1:])[0]\n",
    "    best_idx = torch.argmax(similarities).item()\n",
    "    best_sentence = candidates[best_idx]\n",
    "\n",
    "    if best_sentence.strip().lower() == text.strip().lower():\n",
    "        return None\n",
    "    return best_sentence\n",
    "\n",
    "# =========================\n",
    "# Augmentation Loop\n",
    "# =========================\n",
    "augmented_rows = []\n",
    "repeat_df = subset.sample(needed, replace=True).reset_index(drop=True)\n",
    "\n",
    "for _, row in tqdm(repeat_df.iterrows(), total=needed, desc=f\"Augmenting {label_target}\"):\n",
    "    src_text = row['clean_text']\n",
    "\n",
    "    retry_count = 0\n",
    "    aug_text = paraphrase_text(src_text)\n",
    "\n",
    "    while aug_text is not None and aug_text.strip().lower() == src_text.strip().lower() and retry_count < 2:\n",
    "        aug_text = paraphrase_text(src_text)\n",
    "        retry_count += 1\n",
    "\n",
    "    if aug_text is None or aug_text.strip().lower() == src_text.strip().lower():\n",
    "        continue\n",
    "\n",
    "    augmented_rows.append({\n",
    "        \"text\": row['text'],\n",
    "        \"clean_text\": aug_text,\n",
    "        \"label\": label_target\n",
    "    })\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_rows)\n",
    "augmented_df.to_csv(output_file, index=False)\n",
    "print(f\"âœ… Augmentasi selesai, hasil disimpan di {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42fa0ee",
   "metadata": {},
   "source": [
    "## Pertahanan dan Keamanan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcecf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "label_target = \"Pertahanan dan Keamanan\"\n",
    "output_file = f\"prepData_{label_target.replace(' ', '_')}.csv\"\n",
    "\n",
    "majority_class = data['label'].value_counts().idxmax()\n",
    "target_count = data['label'].value_counts()[majority_class]\n",
    "\n",
    "subset = data[data['label'] == label_target]\n",
    "count = len(subset)\n",
    "needed = target_count - count\n",
    "print(f\"Augmentasi kelas {label_target} -> {needed} data\")\n",
    "\n",
    "if needed <= 0:\n",
    "    raise SystemExit(\"Tidak perlu augmentasi.\")\n",
    "\n",
    "# =========================\n",
    "# Device Setting (GPU Kaggle)\n",
    "# =========================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ“Œ Menggunakan device: {device}\")\n",
    "\n",
    "# =========================\n",
    "# Load Models\n",
    "# =========================\n",
    "model_path = \"data/indoT5-paraphrase\"\n",
    "tokenizer_para = AutoTokenizer.from_pretrained(model_path)\n",
    "model_para = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "sim_model = SentenceTransformer('data/distiluse', device=device)\n",
    "\n",
    "# =========================\n",
    "# Fungsi Paraphrase IndoT5\n",
    "# =========================\n",
    "def paraphrase_text(text):\n",
    "    inputs = tokenizer_para(\n",
    "        f\"parafrase: {text}\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model_para.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=256,\n",
    "        num_return_sequences=3,\n",
    "        do_sample=True,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "    candidates = tokenizer_para.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    embeddings = sim_model.encode([text] + candidates, convert_to_tensor=True, device=device)\n",
    "    similarities = util.cos_sim(embeddings[0], embeddings[1:])[0]\n",
    "    best_idx = torch.argmax(similarities).item()\n",
    "    best_sentence = candidates[best_idx]\n",
    "\n",
    "    if best_sentence.strip().lower() == text.strip().lower():\n",
    "        return None\n",
    "    return best_sentence\n",
    "\n",
    "# =========================\n",
    "# Augmentation Loop\n",
    "# =========================\n",
    "augmented_rows = []\n",
    "repeat_df = subset.sample(needed, replace=True).reset_index(drop=True)\n",
    "\n",
    "for _, row in tqdm(repeat_df.iterrows(), total=needed, desc=f\"Augmenting {label_target}\"):\n",
    "    src_text = row['clean_text']\n",
    "\n",
    "    retry_count = 0\n",
    "    aug_text = paraphrase_text(src_text)\n",
    "\n",
    "    while aug_text is not None and aug_text.strip().lower() == src_text.strip().lower() and retry_count < 2:\n",
    "        aug_text = paraphrase_text(src_text)\n",
    "        retry_count += 1\n",
    "\n",
    "    if aug_text is None or aug_text.strip().lower() == src_text.strip().lower():\n",
    "        continue\n",
    "\n",
    "    augmented_rows.append({\n",
    "        \"text\": row['text'],\n",
    "        \"clean_text\": aug_text,\n",
    "        \"label\": label_target\n",
    "    })\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_rows)\n",
    "augmented_df.to_csv(output_file, index=False)\n",
    "print(f\"âœ… Augmentasi selesai, hasil disimpan di {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85794412",
   "metadata": {},
   "source": [
    "## Ideologi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01eaadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "label_target = \"Ideologi\"\n",
    "output_file = f\"prepData_{label_target.replace(' ', '_')}.csv\"\n",
    "\n",
    "majority_class = data['label'].value_counts().idxmax()\n",
    "target_count = data['label'].value_counts()[majority_class]\n",
    "\n",
    "subset = data[data['label'] == label_target]\n",
    "count = len(subset)\n",
    "needed = target_count - count\n",
    "print(f\"Augmentasi kelas {label_target} -> {needed} data\")\n",
    "\n",
    "if needed <= 0:\n",
    "    raise SystemExit(\"Tidak perlu augmentasi.\")\n",
    "\n",
    "# =========================\n",
    "# Device Setting (GPU Kaggle)\n",
    "# =========================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ“Œ Menggunakan device: {device}\")\n",
    "\n",
    "# =========================\n",
    "# Load Models\n",
    "# =========================\n",
    "model_path = \"data/indoT5-paraphrase\"\n",
    "tokenizer_para = AutoTokenizer.from_pretrained(model_path)\n",
    "model_para = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "sim_model = SentenceTransformer('data/distiluse', device=device)\n",
    "\n",
    "# =========================\n",
    "# Fungsi Paraphrase IndoT5\n",
    "# =========================\n",
    "def paraphrase_text(text):\n",
    "    inputs = tokenizer_para(\n",
    "        f\"parafrase: {text}\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model_para.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=256,\n",
    "        num_return_sequences=3,\n",
    "        do_sample=True,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "    candidates = tokenizer_para.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    embeddings = sim_model.encode([text] + candidates, convert_to_tensor=True, device=device)\n",
    "    similarities = util.cos_sim(embeddings[0], embeddings[1:])[0]\n",
    "    best_idx = torch.argmax(similarities).item()\n",
    "    best_sentence = candidates[best_idx]\n",
    "\n",
    "    if best_sentence.strip().lower() == text.strip().lower():\n",
    "        return None\n",
    "    return best_sentence\n",
    "\n",
    "# =========================\n",
    "# Augmentation Loop\n",
    "# =========================\n",
    "augmented_rows = []\n",
    "repeat_df = subset.sample(needed, replace=True).reset_index(drop=True)\n",
    "\n",
    "for _, row in tqdm(repeat_df.iterrows(), total=needed, desc=f\"Augmenting {label_target}\"):\n",
    "    src_text = row['clean_text']\n",
    "\n",
    "    retry_count = 0\n",
    "    aug_text = paraphrase_text(src_text)\n",
    "\n",
    "    while aug_text is not None and aug_text.strip().lower() == src_text.strip().lower() and retry_count < 2:\n",
    "        aug_text = paraphrase_text(src_text)\n",
    "        retry_count += 1\n",
    "\n",
    "    if aug_text is None or aug_text.strip().lower() == src_text.strip().lower():\n",
    "        continue\n",
    "\n",
    "    augmented_rows.append({\n",
    "        \"text\": row['text'],\n",
    "        \"clean_text\": aug_text,\n",
    "        \"label\": label_target\n",
    "    })\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_rows)\n",
    "augmented_df.to_csv(output_file, index=False)\n",
    "print(f\"âœ… Augmentasi selesai, hasil disimpan di {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17605cad",
   "metadata": {},
   "source": [
    "## Ekonomi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179cd557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "label_target = \"Ekonomi\"\n",
    "output_file = f\"prepData_{label_target.replace(' ', '_')}.csv\"\n",
    "\n",
    "majority_class = data['label'].value_counts().idxmax()\n",
    "target_count = data['label'].value_counts()[majority_class]\n",
    "\n",
    "subset = data[data['label'] == label_target]\n",
    "count = len(subset)\n",
    "needed = target_count - count\n",
    "print(f\"Augmentasi kelas {label_target} -> {needed} data\")\n",
    "\n",
    "if needed <= 0:\n",
    "    raise SystemExit(\"Tidak perlu augmentasi.\")\n",
    "\n",
    "# =========================\n",
    "# Device Setting (GPU Kaggle)\n",
    "# =========================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ“Œ Menggunakan device: {device}\")\n",
    "\n",
    "# =========================\n",
    "# Load Models\n",
    "# =========================\n",
    "model_path = \"data/indoT5-paraphrase\"\n",
    "tokenizer_para = AutoTokenizer.from_pretrained(model_path)\n",
    "model_para = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "sim_model = SentenceTransformer('data/distiluse', device=device)\n",
    "\n",
    "# =========================\n",
    "# Fungsi Paraphrase IndoT5\n",
    "# =========================\n",
    "def paraphrase_text(text):\n",
    "    inputs = tokenizer_para(\n",
    "        f\"parafrase: {text}\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model_para.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=256,\n",
    "        num_return_sequences=3,\n",
    "        do_sample=True,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "    candidates = tokenizer_para.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    embeddings = sim_model.encode([text] + candidates, convert_to_tensor=True, device=device)\n",
    "    similarities = util.cos_sim(embeddings[0], embeddings[1:])[0]\n",
    "    best_idx = torch.argmax(similarities).item()\n",
    "    best_sentence = candidates[best_idx]\n",
    "\n",
    "    if best_sentence.strip().lower() == text.strip().lower():\n",
    "        return None\n",
    "    return best_sentence\n",
    "\n",
    "# =========================\n",
    "# Augmentation Loop\n",
    "# =========================\n",
    "augmented_rows = []\n",
    "repeat_df = subset.sample(needed, replace=True).reset_index(drop=True)\n",
    "\n",
    "for _, row in tqdm(repeat_df.iterrows(), total=needed, desc=f\"Augmenting {label_target}\"):\n",
    "    src_text = row['clean_text']\n",
    "\n",
    "    retry_count = 0\n",
    "    aug_text = paraphrase_text(src_text)\n",
    "\n",
    "    while aug_text is not None and aug_text.strip().lower() == src_text.strip().lower() and retry_count < 2:\n",
    "        aug_text = paraphrase_text(src_text)\n",
    "        retry_count += 1\n",
    "\n",
    "    if aug_text is None or aug_text.strip().lower() == src_text.strip().lower():\n",
    "        continue\n",
    "\n",
    "    augmented_rows.append({\n",
    "        \"text\": row['text'],\n",
    "        \"clean_text\": aug_text,\n",
    "        \"label\": label_target\n",
    "    })\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_rows)\n",
    "augmented_df.to_csv(output_file, index=False)\n",
    "print(f\"âœ… Augmentasi selesai, hasil disimpan di {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74bc867",
   "metadata": {},
   "source": [
    "## Sumber Daya Alam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "label_target = \"Sumber Daya Alam\"\n",
    "output_file = f\"prepData_{label_target.replace(' ', '_')}.csv\"\n",
    "\n",
    "majority_class = data['label'].value_counts().idxmax()\n",
    "target_count = data['label'].value_counts()[majority_class]\n",
    "\n",
    "subset = data[data['label'] == label_target]\n",
    "count = len(subset)\n",
    "needed = target_count - count\n",
    "print(f\"Augmentasi kelas {label_target} -> {needed} data\")\n",
    "\n",
    "if needed <= 0:\n",
    "    raise SystemExit(\"Tidak perlu augmentasi.\")\n",
    "\n",
    "# =========================\n",
    "# Device Setting (GPU Kaggle)\n",
    "# =========================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ“Œ Menggunakan device: {device}\")\n",
    "\n",
    "# =========================\n",
    "# Load Models\n",
    "# =========================\n",
    "model_path = \"data/indoT5-paraphrase\"\n",
    "tokenizer_para = AutoTokenizer.from_pretrained(model_path)\n",
    "model_para = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "sim_model = SentenceTransformer('data/distiluse', device=device)\n",
    "\n",
    "# =========================\n",
    "# Fungsi Paraphrase IndoT5\n",
    "# =========================\n",
    "def paraphrase_text(text):\n",
    "    inputs = tokenizer_para(\n",
    "        f\"parafrase: {text}\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model_para.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=256,\n",
    "        num_return_sequences=3,\n",
    "        do_sample=True,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "    candidates = tokenizer_para.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    embeddings = sim_model.encode([text] + candidates, convert_to_tensor=True, device=device)\n",
    "    similarities = util.cos_sim(embeddings[0], embeddings[1:])[0]\n",
    "    best_idx = torch.argmax(similarities).item()\n",
    "    best_sentence = candidates[best_idx]\n",
    "\n",
    "    if best_sentence.strip().lower() == text.strip().lower():\n",
    "        return None\n",
    "    return best_sentence\n",
    "\n",
    "# =========================\n",
    "# Augmentation Loop\n",
    "# =========================\n",
    "augmented_rows = []\n",
    "repeat_df = subset.sample(needed, replace=True).reset_index(drop=True)\n",
    "\n",
    "for _, row in tqdm(repeat_df.iterrows(), total=needed, desc=f\"Augmenting {label_target}\"):\n",
    "    src_text = row['clean_text']\n",
    "\n",
    "    retry_count = 0\n",
    "    aug_text = paraphrase_text(src_text)\n",
    "\n",
    "    while aug_text is not None and aug_text.strip().lower() == src_text.strip().lower() and retry_count < 2:\n",
    "        aug_text = paraphrase_text(src_text)\n",
    "        retry_count += 1\n",
    "\n",
    "    if aug_text is None or aug_text.strip().lower() == src_text.strip().lower():\n",
    "        continue\n",
    "\n",
    "    augmented_rows.append({\n",
    "        \"text\": row['text'],\n",
    "        \"clean_text\": aug_text,\n",
    "        \"label\": label_target\n",
    "    })\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_rows)\n",
    "augmented_df.to_csv(output_file, index=False)\n",
    "print(f\"âœ… Augmentasi selesai, hasil disimpan di {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43126148",
   "metadata": {},
   "source": [
    "## Demografi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "label_target = \"Demografi\"\n",
    "output_file = f\"prepData_{label_target.replace(' ', '_')}.csv\"\n",
    "\n",
    "majority_class = data['label'].value_counts().idxmax()\n",
    "target_count = data['label'].value_counts()[majority_class]\n",
    "\n",
    "subset = data[data['label'] == label_target]\n",
    "count = len(subset)\n",
    "needed = target_count - count\n",
    "print(f\"Augmentasi kelas {label_target} -> {needed} data\")\n",
    "\n",
    "if needed <= 0:\n",
    "    raise SystemExit(\"Tidak perlu augmentasi.\")\n",
    "\n",
    "# =========================\n",
    "# Device Setting (GPU Kaggle)\n",
    "# =========================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ“Œ Menggunakan device: {device}\")\n",
    "\n",
    "# =========================\n",
    "# Load Models\n",
    "# =========================\n",
    "model_path = \"data/indoT5-paraphrase\"\n",
    "tokenizer_para = AutoTokenizer.from_pretrained(model_path)\n",
    "model_para = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "sim_model = SentenceTransformer('data/distiluse', device=device)\n",
    "\n",
    "# =========================\n",
    "# Fungsi Paraphrase IndoT5\n",
    "# =========================\n",
    "def paraphrase_text(text):\n",
    "    inputs = tokenizer_para(\n",
    "        f\"parafrase: {text}\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model_para.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=256,\n",
    "        num_return_sequences=3,\n",
    "        do_sample=True,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "    candidates = tokenizer_para.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    embeddings = sim_model.encode([text] + candidates, convert_to_tensor=True, device=device)\n",
    "    similarities = util.cos_sim(embeddings[0], embeddings[1:])[0]\n",
    "    best_idx = torch.argmax(similarities).item()\n",
    "    best_sentence = candidates[best_idx]\n",
    "\n",
    "    if best_sentence.strip().lower() == text.strip().lower():\n",
    "        return None\n",
    "    return best_sentence\n",
    "\n",
    "# =========================\n",
    "# Augmentation Loop\n",
    "# =========================\n",
    "augmented_rows = []\n",
    "repeat_df = subset.sample(needed, replace=True).reset_index(drop=True)\n",
    "\n",
    "for _, row in tqdm(repeat_df.iterrows(), total=needed, desc=f\"Augmenting {label_target}\"):\n",
    "    src_text = row['clean_text']\n",
    "\n",
    "    retry_count = 0\n",
    "    aug_text = paraphrase_text(src_text)\n",
    "\n",
    "    while aug_text is not None and aug_text.strip().lower() == src_text.strip().lower() and retry_count < 2:\n",
    "        aug_text = paraphrase_text(src_text)\n",
    "        retry_count += 1\n",
    "\n",
    "    if aug_text is None or aug_text.strip().lower() == src_text.strip().lower():\n",
    "        continue\n",
    "\n",
    "    augmented_rows.append({\n",
    "        \"text\": row['text'],\n",
    "        \"clean_text\": aug_text,\n",
    "        \"label\": label_target\n",
    "    })\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_rows)\n",
    "augmented_df.to_csv(output_file, index=False)\n",
    "print(f\"âœ… Augmentasi selesai, hasil disimpan di {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f7a912",
   "metadata": {},
   "source": [
    "## Geografi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed3da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "label_target = \"Geografi\"\n",
    "output_file = f\"prepData_{label_target.replace(' ', '_')}.csv\"\n",
    "\n",
    "majority_class = data['label'].value_counts().idxmax()\n",
    "target_count = data['label'].value_counts()[majority_class]\n",
    "\n",
    "subset = data[data['label'] == label_target]\n",
    "count = len(subset)\n",
    "needed = target_count - count\n",
    "print(f\"Augmentasi kelas {label_target} -> {needed} data\")\n",
    "\n",
    "if needed <= 0:\n",
    "    raise SystemExit(\"Tidak perlu augmentasi.\")\n",
    "\n",
    "# =========================\n",
    "# Device Setting (GPU Kaggle)\n",
    "# =========================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ“Œ Menggunakan device: {device}\")\n",
    "\n",
    "# =========================\n",
    "# Load Models\n",
    "# =========================\n",
    "model_path = \"data/indoT5-paraphrase\"\n",
    "tokenizer_para = AutoTokenizer.from_pretrained(model_path)\n",
    "model_para = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "sim_model = SentenceTransformer('data/distiluse', device=device)\n",
    "\n",
    "# =========================\n",
    "# Fungsi Paraphrase IndoT5\n",
    "# =========================\n",
    "def paraphrase_text(text):\n",
    "    inputs = tokenizer_para(\n",
    "        f\"parafrase: {text}\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model_para.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=256,\n",
    "        num_return_sequences=3,\n",
    "        do_sample=True,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "    candidates = tokenizer_para.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    embeddings = sim_model.encode([text] + candidates, convert_to_tensor=True, device=device)\n",
    "    similarities = util.cos_sim(embeddings[0], embeddings[1:])[0]\n",
    "    best_idx = torch.argmax(similarities).item()\n",
    "    best_sentence = candidates[best_idx]\n",
    "\n",
    "    if best_sentence.strip().lower() == text.strip().lower():\n",
    "        return None\n",
    "    return best_sentence\n",
    "\n",
    "# =========================\n",
    "# Augmentation Loop\n",
    "# =========================\n",
    "augmented_rows = []\n",
    "repeat_df = subset.sample(needed, replace=True).reset_index(drop=True)\n",
    "\n",
    "for _, row in tqdm(repeat_df.iterrows(), total=needed, desc=f\"Augmenting {label_target}\"):\n",
    "    src_text = row['clean_text']\n",
    "\n",
    "    retry_count = 0\n",
    "    aug_text = paraphrase_text(src_text)\n",
    "\n",
    "    while aug_text is not None and aug_text.strip().lower() == src_text.strip().lower() and retry_count < 2:\n",
    "        aug_text = paraphrase_text(src_text)\n",
    "        retry_count += 1\n",
    "\n",
    "    if aug_text is None or aug_text.strip().lower() == src_text.strip().lower():\n",
    "        continue\n",
    "\n",
    "    augmented_rows.append({\n",
    "        \"text\": row['text'],\n",
    "        \"clean_text\": aug_text,\n",
    "        \"label\": label_target\n",
    "    })\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_rows)\n",
    "augmented_df.to_csv(output_file, index=False)\n",
    "print(f\"âœ… Augmentasi selesai, hasil disimpan di {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701c9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "aug_files = glob.glob(\"prepData_*.csv\")\n",
    "augmented_list = [pd.read_csv(file) for file in aug_files]\n",
    "augmented_all = pd.concat(augmented_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e405d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = pd.concat([data, augmented_all], ignore_index=True)\n",
    "final_dataset.to_csv(\"augmentedDataset1.csv\", index=False)\n",
    "\n",
    "print(\"Dataset asli + augmentasi semua kelas disimpan ke augmentedDataset1.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
